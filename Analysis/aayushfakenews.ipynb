{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:29:45.901415Z","iopub.execute_input":"2022-04-02T23:29:45.901749Z","iopub.status.idle":"2022-04-02T23:29:46.21496Z","shell.execute_reply.started":"2022-04-02T23:29:45.901717Z","shell.execute_reply":"2022-04-02T23:29:46.214203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pandas\nimport numpy as numpy\nimport tensorflow as tf\n\nfrom keras_preprocessing.text import Tokenizer\nfrom gensim.models import KeyedVectors\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras import Sequential, Model\nfrom keras.layers import Conv1D, Dropout, Dense, Embedding, MaxPooling1D, Concatenate, Flatten, Input, LSTM\nfrom keras.layers.merge import concatenate\nfrom sklearn.utils import resample","metadata":{"id":"djCHEAfnA1Gz","execution":{"iopub.status.busy":"2022-04-02T16:39:20.080626Z","iopub.execute_input":"2022-04-02T16:39:20.081079Z","iopub.status.idle":"2022-04-02T16:39:25.915616Z","shell.execute_reply.started":"2022-04-02T16:39:20.081009Z","shell.execute_reply":"2022-04-02T16:39:25.914921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '../input/fake-news-challenge/'\n","metadata":{"id":"Fn5jrC19EY4q","execution":{"iopub.status.busy":"2022-04-02T16:41:17.237747Z","iopub.execute_input":"2022-04-02T16:41:17.238025Z","iopub.status.idle":"2022-04-02T16:41:17.243042Z","shell.execute_reply.started":"2022-04-02T16:41:17.237994Z","shell.execute_reply":"2022-04-02T16:41:17.240683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42 # A random seed is a starting point in generating random numbers\nnumpy.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n# With the seed reset (every time), the same set of numbers will appear every time - make the results more predictable and therefore reproducable \n# If the random seed is not reset, different numbers appear with every invocation.","metadata":{"id":"oM_mOsgGBvYH","execution":{"iopub.status.busy":"2022-04-02T16:40:04.071039Z","iopub.execute_input":"2022-04-02T16:40:04.071922Z","iopub.status.idle":"2022-04-02T16:40:04.07685Z","shell.execute_reply.started":"2022-04-02T16:40:04.071875Z","shell.execute_reply":"2022-04-02T16:40:04.07551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# load the data set from the train csv files\ndef load_train_data():     \n    #create Pandas dataframes from the two csv files\n    train_bodies = pandas.read_csv(PATH + \"train_bodies.csv\", encoding='utf-8')\n    train_headlines = pandas.read_csv(PATH + \"train_stances.csv\", encoding='utf-8')\n\n    #merge the csv files on Body ID so that we can have article headlines, bodies, and stances all in one dataframe together \n    train_data_set = pandas.merge(train_bodies, train_headlines, how='left', on='Body ID')\n    stances = {\n        'Stance': {\n          'agree': 0,\n          'disagree': 1,\n          'discuss': 2,\n          'unrelated': 3,\n        }\n    }\n    train_data_set.replace(stances, inplace=True)\n    print(\"original here\")\n    print(train_data_set)\n    # print(train_data_set['Stance'].value_counts())\n   \n    # average to 8909 or 3678? because 36545 is a lot but 840 is very small\n    # unrelated - 36545, discuss - 8909, agree - 3678, disagree - 840\n    data_length = 8909\n\n    # resampling ensures that distribution of classes (in this case, stances) are even - we chose to match them to one of the middle distribution sizes \n    # we want to pversample to better represent minorities classes (agree and disagree) so model has more samples to learn more \n    # also want to undersample majority class (unrelated) so that we don't risk our model skewing towards this class \n    unrelated_resampled = resample(train_data_set.loc[train_data_set['Stance'] == 3], replace = False, n_samples = data_length, random_state = RANDOM_SEED)\n    discuss_resampled = resample(train_data_set.loc[train_data_set['Stance'] == 2], replace = False, n_samples = data_length, random_state = RANDOM_SEED)\n    agree_resampled = resample(train_data_set.loc[train_data_set['Stance'] == 0], replace=True, n_samples=data_length, random_state=RANDOM_SEED)\n    disagree_resampled = resample(train_data_set.loc[train_data_set['Stance'] == 1], replace=True, n_samples=data_length, random_state=RANDOM_SEED)\n  \n    \n    all_resampled = [unrelated_resampled, discuss_resampled, agree_resampled, disagree_resampled]\n    result = pandas.concat(all_resampled)\n    result = result.sample(frac=1)\n\n    print(result['Stance'].value_counts())\n    print(\"result here\")\n    print(result)\n\n    return result","metadata":{"id":"wYq7PhTIB1fi","execution":{"iopub.status.busy":"2022-04-02T16:42:21.813792Z","iopub.execute_input":"2022-04-02T16:42:21.814041Z","iopub.status.idle":"2022-04-02T16:42:21.829079Z","shell.execute_reply.started":"2022-04-02T16:42:21.814013Z","shell.execute_reply":"2022-04-02T16:42:21.828396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_test_data():     \n    #create Pandas dataframes from the two csv files\n    train_bodies = pandas.read_csv(PATH + \"competition_test_bodies.csv\", encoding='utf-8')\n    train_headlines = pandas.read_csv('../input/competition-test-stances/competition_test_stances.csv', encoding='utf-8')\n\n    #merge the csv files on Body ID\n    test_data_set = pandas.merge(train_bodies, train_headlines, how='left', on='Body ID')\n    stances = {\n        'Stance': {\n          'agree': 0,\n          'disagree': 1,\n          'discuss': 2,\n          'unrelated': 3,\n        }\n    }\n    test_data_set.replace(stances, inplace=True)\n    print(test_data_set['Stance'].value_counts())\n    # print(test_data_set)\n    \n    return test_data_set \n","metadata":{"id":"D1_O0vVFGoSE","execution":{"iopub.status.busy":"2022-04-02T16:55:03.437945Z","iopub.execute_input":"2022-04-02T16:55:03.438529Z","iopub.status.idle":"2022-04-02T16:55:03.444129Z","shell.execute_reply.started":"2022-04-02T16:55:03.438489Z","shell.execute_reply":"2022-04-02T16:55:03.443464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle as pkl\n\ndef prepare_data(data_set, length=None):\n    # find the max length of each dataset, this is used to pad sequence vectors below so that we can safely pass data without worrying about dimensions\n    bodies_max_length = 0\n    headlines_max_length = 0\n    if not length:\n      bodies_max_length = data_set['articleBody'].map(lambda x : len(x.split())).max()\n      headlines_max_length = data_set['Headline'].map(lambda x : len(x.split())).max()\n    else:\n      bodies_max_length = length[0]\n      headlines_max_length = length[1]\n\n    # we want to tokenize the data to break down the text into smaller units called tokens (can be words, characters, or subwords)\n    # tokens are then used to prepare a vocabulary of the dataset (vocab is the set of unique tokens in the corpus)\n    bodies_tokenizer, headlines_tokenizer = (Tokenizer(), Tokenizer())\n    \n    # fit the tokenizer on the data - creates the vocabulary index based on word frequency\n    # the lower the index, the more frequently it appears\n    bodies_tokenizer.fit_on_texts(data_set['articleBody'])\n    headlines_tokenizer.fit_on_texts(data_set['Headline'])\n\n    with open('bodies_tokenizer.pkl', 'wb') as handle:\n      pkl.dump(bodies_tokenizer, handle, protocol=pkl.HIGHEST_PROTOCOL)\n    with open('headlines_tokenizer.pkl', 'wb') as handle:\n      pkl.dump(headlines_tokenizer, handle, protocol=pkl.HIGHEST_PROTOCOL)\n    \n    # convert the texts to sequences, we need to do this because computers understand integers not strings \n    # this process ransforms each text in texts to a sequence of integers, essentially it takes each word in the text and replaces it with its corresponding integer value from the vocab\n    bodies_sequences = bodies_tokenizer.texts_to_sequences(data_set['articleBody'])\n    headlines_sequences = headlines_tokenizer.texts_to_sequences(data_set['Headline'])\n\n    # pad the data to be the max length, this helps avoid dimension errors \n    bodies_sequences = pad_sequences(bodies_sequences, maxlen=bodies_max_length, padding='post', truncating='post')\n    headlines_sequences = pad_sequences(headlines_sequences, maxlen=headlines_max_length, padding='post', truncating='post')\n\n\n    return bodies_sequences, headlines_sequences, bodies_tokenizer.word_index, headlines_tokenizer.word_index, data_set['Stance']\n\n\ndef create_embeddings(bodies_word_index, headlines_word_index):\n    # create empty dictionaries for the embeddings\n    bodies_embeddings_index, headlines_embeddings_index = ({},{})\n    word2vec_model = KeyedVectors.load_word2vec_format('../input/googlenewsvectors/' + \"GoogleNews-vectors-negative300.bin\", binary=True) # here we are using \n\n    def getVector(str):\n      if str in word2vec_model:\n        return word2vec_model[str]\n      else:\n        return None;\n\n    #save the vector for each word to the matrix\n    bodies_embeddings_matrix = numpy.zeros((len(bodies_word_index)+1, 300))\n    for word, i in bodies_word_index.items():\n        embedding_vector = getVector(word)\n        if embedding_vector is not None:\n            bodies_embeddings_matrix[i] = embedding_vector\n\n    headlines_embeddings_matrix = numpy.zeros((len(headlines_word_index)+1, 300))\n    for word, i in headlines_word_index.items():\n        embedding_vector = getVector(word)\n        if embedding_vector is not None:\n            headlines_embeddings_matrix[i] = embedding_vector\n\n    return bodies_embeddings_matrix, headlines_embeddings_matrix\n    \n    #save the vector for each word to the matrix\n    bodies_embeddings_matrix = numpy.zeros((len(bodies_word_index)+1, 100))\n    for word, i in bodies_word_index.items():\n        embedding_vector = bodies_embeddings_index.get(word)\n        if embedding_vector is not None:\n            bodies_embeddings_matrix[i] = embedding_vector\n\n    headlines_embeddings_matrix = numpy.zeros((len(headlines_word_index)+1, 100))\n    for word, i in headlines_word_index.items():\n        embedding_vector = headlines_embeddings_index.get(word)\n        if embedding_vector is not None:\n            headlines_embeddings_matrix[i] = embedding_vector\n\n    return bodies_embeddings_matrix, headlines_embeddings_matrix\n\nif __name__ == '__main__':\n    train_data = load_train_data()\n    # train_data = train_data[train_data['Stance'] != 3]\n\n    # g = train_data.groupby('Stance')\n    # train_data = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n\n\n    test_data = load_test_data()\n\n    # f = test_data.groupby('Stance')\n    # test_data = f.apply(lambda x: x.sample(f.size().min()).reset_index(drop=True))\n    # test_data = test_data[test_data['Stance'] != 3]\n    \n    bodies_sequences, headlines_sequences, bodies_word_index, headlines_word_index, stances = prepare_data(train_data)\n    test_bodies_sequences, test_headlines_sequences, test_bodies_word_index, test_headlines_word_index, test_stances = prepare_data(test_data,[bodies_sequences.shape[1],headlines_sequences.shape[1]])\n\n    \n    bodies_embeddings_matrix, headlines_embeddings_matrix = create_embeddings(bodies_word_index=bodies_word_index, headlines_word_index=headlines_word_index)\n\n    bodies_vocab_size, headlines_vocab_size = len(bodies_word_index), len(headlines_word_index)\n","metadata":{"id":"b5GKoDa7GWcn","outputId":"decdd82b-13d5-44f9-ae0b-5531529d1463","execution":{"iopub.status.busy":"2022-04-02T16:57:54.036033Z","iopub.execute_input":"2022-04-02T16:57:54.036299Z","iopub.status.idle":"2022-04-02T16:59:43.087204Z","shell.execute_reply.started":"2022-04-02T16:57:54.036267Z","shell.execute_reply":"2022-04-02T16:59:43.084494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(embedding_matrix, vocab_size, input_length):\n    model = Sequential()  # stack of layers, each has one input and one output that are passed through layers \n    model.add(Embedding(vocab_size + 1,300, weights = [embedding_matrix], trainable=False, input_length=input_length)) # tell model to use word2vec embeddings for words\n\n    # convolutional -> dropout -> pooling\n    \n    # convolutional: apply a filter to an input to create a feature map that summarizes the presence of detected features in the input\n    # dropout: randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting\n      # overfitting means that the model learns features of the testing set too well that it can't generalize to testing data and has low testing accuracy but high training accuracy\n    # pooling: reduce the dimensions of the feature maps -> reduce number of parameters to learn by summarizing the features present in a region of feature map \n\n    model.add(Conv1D(256, 3, activation='relu')) # activation function decides if node would fire or not - relu gets rid of negative inputs \n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2, padding=\"same\"))\n    model.add(LSTM(100, activation='tanh', return_sequences=True))\n\n    model.add(Conv1D(256, 3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2,padding=\"same\"))\n    model.add(LSTM(100, activation='tanh', return_sequences=True))\n\n    model.add(Conv1D(512, 3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2,padding=\"same\"))\n    model.add(LSTM(100, activation='tanh', return_sequences=True))\n\n    model.add(Conv1D(512, 3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2,padding=\"same\"))\n    model.add(LSTM(100, activation='tanh', return_sequences=True))\n\n\n    print(input_length)\n    if input_length >= 512:\n      print(\"issue5 starting\")\n      model.add(Conv1D(512, 3, activation='relu'))\n      model.add(Dropout(0.5))\n      model.add(MaxPooling1D(pool_size=2,padding=\"same\"))\n      model.add(LSTM(100, activation='tanh', return_sequences=True))\n      print(\"issue5\")\n      \n\n    print(\"issue6 starting\")\n    model.add(Conv1D(768, 1, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2,padding=\"same\"))\n    model.add(LSTM(100, activation='tanh', return_sequences=True))\n\n    model.add(Flatten())\n\n    return model","metadata":{"id":"MWhsAebOCJN4","execution":{"iopub.status.busy":"2022-04-02T16:59:57.817298Z","iopub.execute_input":"2022-04-02T16:59:57.817547Z","iopub.status.idle":"2022-04-02T16:59:57.833348Z","shell.execute_reply.started":"2022-04-02T16:59:57.817517Z","shell.execute_reply":"2022-04-02T16:59:57.832385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bodies_model = create_model(embedding_matrix=bodies_embeddings_matrix, vocab_size=bodies_vocab_size, input_length=bodies_sequences.shape[1])\n\nheadlines_model = create_model(embedding_matrix=headlines_embeddings_matrix, vocab_size=headlines_vocab_size, input_length=headlines_sequences.shape[1])\n\nprint(bodies_vocab_size)\nprint(headlines_vocab_size)\n\n\nfinalModel = Sequential()\nprint(bodies_model.input)\nprint(headlines_model.input)\nprint(bodies_model.output)\nprint(headlines_model.output)\nfinalModel = Concatenate()([bodies_model.output, headlines_model.output])\nfinalModel = Flatten()(finalModel)\nfinalModel = Dense(1024, activation='relu') (finalModel)\nfinalModel = Dense(1024, activation='relu') (finalModel)\nfinalModel = Dense(1024, activation='relu') (finalModel)\nfinalModel = Dense(4, activation='softmax') (finalModel)\n#0,1,2,3\n#0: [1,0,0,0]\n#1: [0,1,0,0]\n#2: [0,0,1,0]\n\nmodel = Model(inputs=[bodies_model.input, headlines_model.input], outputs = finalModel)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n\n\nprint(model.summary())\nfrom keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file= './' + 'model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"id":"Q1TGU7EPCT01","outputId":"536d7f75-078d-4ba0-a534-7f341aadb603","execution":{"iopub.status.busy":"2022-04-02T17:01:15.577973Z","iopub.execute_input":"2022-04-02T17:01:15.578876Z","iopub.status.idle":"2022-04-02T17:01:18.464094Z","shell.execute_reply.started":"2022-04-02T17:01:15.578812Z","shell.execute_reply":"2022-04-02T17:01:18.462464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\nclass PlotLearning(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        self.acc = []\n        self.val_acc = []\n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n        self.i += 1\n        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n        \n        clear_output(wait=True)\n        \n        ax1.set_yscale('log')\n        ax1.plot(self.x, self.losses, label=\"loss\")\n        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n        ax1.legend()\n        \n        ax2.plot(self.x, self.acc, label=\"accuracy\")\n        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n        ax2.legend()\n        \n        plt.show();\n        \nplot = PlotLearning()","metadata":{"id":"MPmcajME_UUX","execution":{"iopub.status.busy":"2022-04-02T17:01:27.713182Z","iopub.execute_input":"2022-04-02T17:01:27.71345Z","iopub.status.idle":"2022-04-02T17:01:27.724224Z","shell.execute_reply.started":"2022-04-02T17:01:27.71342Z","shell.execute_reply":"2022-04-02T17:01:27.723544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath= './' + \"test.best.hdf5\"","metadata":{"id":"zqZdQlrZtO0y","execution":{"iopub.status.busy":"2022-04-02T17:01:32.568906Z","iopub.execute_input":"2022-04-02T17:01:32.569679Z","iopub.status.idle":"2022-04-02T17:01:32.573804Z","shell.execute_reply.started":"2022-04-02T17:01:32.56963Z","shell.execute_reply":"2022-04-02T17:01:32.573065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # print(headlines_sequences[4].size)\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\nprint(bodies_sequences.shape)\nprint(headlines_sequences.shape)\nprint(stances)\nonehot_stances = to_categorical(stances)\nprint(onehot_stances)\n\n\nstances_counts = train_data['Stance'].value_counts();\ncount_total = stances_counts.sum()\ncount_unrelated, count_dicuss, count_agree, count_disagree = stances_counts[3], stances_counts[2], stances_counts[0], stances_counts[1]\nweight_unrelated = 1/(count_unrelated) * (count_total) / 2.0\nweight_discuss = 1/(count_dicuss) * (count_total) / 2.0\nweight_agree = 1/(count_agree) * (count_total) / 2.0\nweight_disagree = 1/(count_disagree) * (count_total) / 2.0\n\nclass_weights = {0: weight_agree, 1: weight_disagree, 2: weight_discuss, 3: weight_unrelated}\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nmodel.fit([bodies_sequences, headlines_sequences],\n              onehot_stances,batch_size=16,\n              epochs=50, callbacks=callbacks_list,\n              validation_split=0.05, \n              shuffle=True,\n              )\n\n\nmodel.save('./' + 'model')\n    ","metadata":{"id":"-BRlCAbMC_ep","outputId":"1c99f560-4289-4310-e717-156d5a834c70","execution":{"iopub.status.busy":"2022-04-02T17:04:46.365584Z","iopub.execute_input":"2022-04-02T17:04:46.366298Z","iopub.status.idle":"2022-04-02T17:04:53.7788Z","shell.execute_reply.started":"2022-04-02T17:04:46.366258Z","shell.execute_reply":"2022-04-02T17:04:53.77777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('./' + \"test.best.hdf5\")\n\nfrom tensorflow.keras.utils import to_categorical\ntest_onehot_stances = to_categorical(test_stances)\nprint(len(bodies_sequences))\nprint(len(test_bodies_sequences), test_headlines_sequences[0], test_onehot_stances[0])\nmodel.evaluate([test_bodies_sequences, test_headlines_sequences], test_onehot_stances)","metadata":{"id":"y6Z7BWpYEHhy","outputId":"3d0b6242-610f-47ce-ae57-f4d4687929ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndef test(headline, body):\n  data = {'Headline': [headline], 'articleBody':[body], 'Stance': [None]}\n  df = pd.DataFrame.from_dict(data)\n  bodies_sequences, headlines_sequences, bodies_word_index, headlines_word_index, stances = prepare_data(df, [2243,40])\n  stances = {\n      0: \"agree\",\n      1: \"disagree\",\n      2: \"discuss\",\n      3: \"unrelated\"\n  }\n  prediction = model.predict([bodies_sequences, headlines_sequences])\n  print(prediction)\n  print(stances[np.argmax(prediction)])\ntest(\"Pope Francis loves Donald Trump\", '''Pope Francis hates Donlad Trump''')","metadata":{"id":"hsmA4gk2EIed"},"execution_count":null,"outputs":[]}]}